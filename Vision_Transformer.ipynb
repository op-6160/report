{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://github.com/FrancescoSaverioZuppichini/ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ViT Architecture\n",
    "<img src=\"https://github.com/FrancescoSaverioZuppichini/ViT/raw/main/images/ViT.png?raw=true\" width=\"500\" height=\"378\">\n",
    "\n",
    "\n",
    "입력 이미지를 P * P 사이즈의 패치로 나눠 flatten 해 1차원 벡터 형태로 Transformer Encoder 에 입력 (Embbeding)\n",
    "Multi Head Attention 후 Feed Forward Layer 까지의 인코더 과정을 거침"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* * *\n",
    "## Patch Embedding\n",
    "#### 이미지를 입력받아 패치사이즈로 나누고 1차원 벡터로 projection 시킨 후 class token 과 positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "배치8 채널3 크기224*224 랜덤텐서 생성\n",
    "Kernal과 Stride를 Patch_size로 갖는 Conv2D를 이용해 flatten\n",
    "```\n",
    "x = torch.randn(8, 3, 224, 224)\n",
    "# x = torch.Size([8, 196, 768])  {batch 8 / channel 3 / 224*224}\n",
    "patch_size = 16\n",
    "in_channels = 3\n",
    "emb_size = 768\n",
    "projection = nn.Sequential(nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "# projection = torch.Size([8, 196, 768])\n",
    "\n",
    "emb_size = 768\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Class Token / Positional Encoding 추가\n",
    "이미지 x 를 fetch로 나누고 flatten\n",
    "```\n",
    "projected_x = projection(x)\n",
    "# projected_x = torch.Size([8, 196, 768]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "cls_token = 무작위로 초기화되는 torch 매개변수, forward에서 batch로 복사되고 torch.cat을 사용해 패치 앞에 추가됨\n",
    "cls_token 생성, Positional Encoding Parameter 정의\n",
    "```\n",
    "cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "# cls_token = torch.Size([1, 1, 768])\n",
    "positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "# positions = torch.Size([197, 768])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "cls_token 을 반복해 배치와 크기를 맞춤\n",
    "```\n",
    "batch_size = 8\n",
    "cls_tokens = repeat(cls_token, '() n e -> b n e', b=batch_size)\n",
    "# cls_token = torch.Size([8, 1, 768])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "cls_token 과 X 를 1차원 연결\n",
    "```\n",
    "cat_x = torch.cat([cls_tokens, projected_x], dim=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "모든 배치에 position encoding 을 더해줌\n",
    "```\n",
    "cat_x += positions\n",
    "# cat_x = torch.Size([8, 197, 768])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PatchEmbedding 을 클래스 형태로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # using a conv layer instead of a linear one -> performance gains\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* * *\n",
    "\n",
    "## Multy Head Attention (MHA)\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\" width=\"309\" height=\"400\">\n",
    "\n",
    "#### 3개의 Linear Projection 을 통해 임베딩된 후, 여러개의 head로 나눠져 각각 Scaled Dot-Product Attention 진행\n",
    "ViT에서 Querie Key Value는 같은 텐서로 입력됨\n",
    "\n",
    "### Linear Projection\n",
    "\n",
    "embedding 된 입력 텐서를 임베딩 사이즈로 Linear Projection\n",
    "K/Q/V 레이어 생성, 각 레이어는 모델 훈련과정에 학습됨\n",
    "\n",
    "```\n",
    "emb_size = 768\n",
    "num_heads = 8\n",
    "\n",
    "keys = nn.Linear(emb_size, emb_size)\n",
    "queries = nn.Linear(emb_size, emb_size)\n",
    "values = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "# K,Q,V = in_features=768, out_features=768, bias=True\n",
    "```\n",
    "### Multi-Head\n",
    "\n",
    "Linear Projection 된 Q/K/V 를 8개의 Multi Head 로 나눔\n",
    "\n",
    "```\n",
    "queries = rearrange(queries(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "keys = rearrange(keys(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "values  = rearrange(values(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "\n",
    "# Q,K,V = torch.Size([8, 8, 197, 96])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Scaled Dot Product Attention\n",
    "Q * K 및 내적\n",
    "```\n",
    "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "\n",
    "# energy = torch.Size([8, 8, 197, 197])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "스케일링 후 Attention Score 와 V 를 내적 , embedding size 로 크기변환\n",
    "```\n",
    "# Get Attention Score\n",
    "scaling = emb_size ** (1/2)\n",
    "att = F.softmax(energy, dim=-1) / scaling\n",
    "# att = torch.Size([8, 8, 197, 197])\n",
    "\n",
    "# Attention Score * values\n",
    "out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "# out = torch.Size([8, 8, 197, 96])\n",
    "\n",
    "out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "# out = torch.Size([8, 197, 768])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Multi Head Attention을 클래스 형태로 구현\n",
    "\n",
    "Q/K/V 각각 1개씩의 Linear 설정 대신 emb_size * 3으로 설정 후 연산시에 분배"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Class 로 구현한 MHA\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* * *\n",
    "## ViT Encoder\n",
    "\n",
    "### Residual Connection\n",
    "<img src=\"https://github.com/FrancescoSaverioZuppichini/ViT/raw/main/images/TransformerBlockAttentionRes.png?raw=true\">\n",
    "\n",
    "fn을 입력받아 사용 fn을 forward 후 res를 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Free Forward MLP\n",
    "<img src=\"https://github.com/FrancescoSaverioZuppichini/ViT/raw/main/images/TransformerBlockAttentionZoom.png?raw=true\" width=\"208\" height=\"400\">\n",
    "\n",
    "Attention 의 output 이 FCL 로 전달됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transformer Encoder 구현\n",
    "<img src=\"https://github.com/FrancescoSaverioZuppichini/ViT/raw/main/images/TransformerBlock.png?raw=true\" width=\"194\" height=\"400\">\n",
    "\n",
    "구현한 클래스들을 결합해 Transformer encoder blcok을 구현하고\n",
    "블록을 depth만큼 중첩시켜 Transformer Encoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Head Layer\n",
    "\n",
    "FCL Classification 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ViT_model = ViT().cuda()\n",
    "\n",
    "#summary(ViT_model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```\n",
    "summary(ViT_model, (3, 224, 224))\n",
    "----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
    "         Rearrange-2             [-1, 196, 768]               0\n",
    "    PatchEmbedding-3             [-1, 197, 768]               0\n",
    "         LayerNorm-4             [-1, 197, 768]           1,536\n",
    "            Linear-5            [-1, 197, 2304]       1,771,776\n",
    "           Dropout-6          [-1, 8, 197, 197]               0\n",
    "            Linear-7             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-8             [-1, 197, 768]               0\n",
    "           Dropout-9             [-1, 197, 768]               0\n",
    "      ResidualAdd-10             [-1, 197, 768]               0\n",
    "        LayerNorm-11             [-1, 197, 768]           1,536\n",
    "           Linear-12            [-1, 197, 3072]       2,362,368\n",
    "             GELU-13            [-1, 197, 3072]               0\n",
    "          Dropout-14            [-1, 197, 3072]               0\n",
    "           Linear-15             [-1, 197, 768]       2,360,064\n",
    "          Dropout-16             [-1, 197, 768]               0\n",
    "      ResidualAdd-17             [-1, 197, 768]               0\n",
    "        LayerNorm-18             [-1, 197, 768]           1,536\n",
    "           Linear-19            [-1, 197, 2304]       1,771,776\n",
    "          Dropout-20          [-1, 8, 197, 197]               0\n",
    "           Linear-21             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-22             [-1, 197, 768]               0\n",
    "          Dropout-23             [-1, 197, 768]               0\n",
    "      ResidualAdd-24             [-1, 197, 768]               0\n",
    "        LayerNorm-25             [-1, 197, 768]           1,536\n",
    "           Linear-26            [-1, 197, 3072]       2,362,368\n",
    "             GELU-27            [-1, 197, 3072]               0\n",
    "          Dropout-28            [-1, 197, 3072]               0\n",
    "           Linear-29             [-1, 197, 768]       2,360,064\n",
    "          Dropout-30             [-1, 197, 768]               0\n",
    "      ResidualAdd-31             [-1, 197, 768]               0\n",
    "        LayerNorm-32             [-1, 197, 768]           1,536\n",
    "           Linear-33            [-1, 197, 2304]       1,771,776\n",
    "          Dropout-34          [-1, 8, 197, 197]               0\n",
    "           Linear-35             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-36             [-1, 197, 768]               0\n",
    "          Dropout-37             [-1, 197, 768]               0\n",
    "      ResidualAdd-38             [-1, 197, 768]               0\n",
    "        LayerNorm-39             [-1, 197, 768]           1,536\n",
    "           Linear-40            [-1, 197, 3072]       2,362,368\n",
    "             GELU-41            [-1, 197, 3072]               0\n",
    "          Dropout-42            [-1, 197, 3072]               0\n",
    "           Linear-43             [-1, 197, 768]       2,360,064\n",
    "          Dropout-44             [-1, 197, 768]               0\n",
    "      ResidualAdd-45             [-1, 197, 768]               0\n",
    "        LayerNorm-46             [-1, 197, 768]           1,536\n",
    "           Linear-47            [-1, 197, 2304]       1,771,776\n",
    "          Dropout-48          [-1, 8, 197, 197]               0\n",
    "           Linear-49             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-50             [-1, 197, 768]               0\n",
    "          Dropout-51             [-1, 197, 768]               0\n",
    "      ResidualAdd-52             [-1, 197, 768]               0\n",
    "        LayerNorm-53             [-1, 197, 768]           1,536\n",
    "           Linear-54            [-1, 197, 3072]       2,362,368\n",
    "             GELU-55            [-1, 197, 3072]               0\n",
    "          Dropout-56            [-1, 197, 3072]               0\n",
    "           Linear-57             [-1, 197, 768]       2,360,064\n",
    "          Dropout-58             [-1, 197, 768]               0\n",
    "      ResidualAdd-59             [-1, 197, 768]               0\n",
    "        LayerNorm-60             [-1, 197, 768]           1,536\n",
    "           Linear-61            [-1, 197, 2304]       1,771,776\n",
    "          Dropout-62          [-1, 8, 197, 197]               0\n",
    "           Linear-63             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-64             [-1, 197, 768]               0\n",
    "          Dropout-65             [-1, 197, 768]               0\n",
    "      ResidualAdd-66             [-1, 197, 768]               0\n",
    "        LayerNorm-67             [-1, 197, 768]           1,536\n",
    "           Linear-68            [-1, 197, 3072]       2,362,368\n",
    "             GELU-69            [-1, 197, 3072]               0\n",
    "          Dropout-70            [-1, 197, 3072]               0\n",
    "           Linear-71             [-1, 197, 768]       2,360,064\n",
    "          Dropout-72             [-1, 197, 768]               0\n",
    "      ResidualAdd-73             [-1, 197, 768]               0\n",
    "        LayerNorm-74             [-1, 197, 768]           1,536\n",
    "           Linear-75            [-1, 197, 2304]       1,771,776\n",
    "          Dropout-76          [-1, 8, 197, 197]               0\n",
    "           Linear-77             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-78             [-1, 197, 768]               0\n",
    "          Dropout-79             [-1, 197, 768]               0\n",
    "      ResidualAdd-80             [-1, 197, 768]               0\n",
    "        LayerNorm-81             [-1, 197, 768]           1,536\n",
    "           Linear-82            [-1, 197, 3072]       2,362,368\n",
    "             GELU-83            [-1, 197, 3072]               0\n",
    "          Dropout-84            [-1, 197, 3072]               0\n",
    "           Linear-85             [-1, 197, 768]       2,360,064\n",
    "          Dropout-86             [-1, 197, 768]               0\n",
    "      ResidualAdd-87             [-1, 197, 768]               0\n",
    "        LayerNorm-88             [-1, 197, 768]           1,536\n",
    "           Linear-89            [-1, 197, 2304]       1,771,776\n",
    "          Dropout-90          [-1, 8, 197, 197]               0\n",
    "           Linear-91             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-92             [-1, 197, 768]               0\n",
    "          Dropout-93             [-1, 197, 768]               0\n",
    "      ResidualAdd-94             [-1, 197, 768]               0\n",
    "        LayerNorm-95             [-1, 197, 768]           1,536\n",
    "           Linear-96            [-1, 197, 3072]       2,362,368\n",
    "             GELU-97            [-1, 197, 3072]               0\n",
    "          Dropout-98            [-1, 197, 3072]               0\n",
    "           Linear-99             [-1, 197, 768]       2,360,064\n",
    "         Dropout-100             [-1, 197, 768]               0\n",
    "     ResidualAdd-101             [-1, 197, 768]               0\n",
    "       LayerNorm-102             [-1, 197, 768]           1,536\n",
    "          Linear-103            [-1, 197, 2304]       1,771,776\n",
    "         Dropout-104          [-1, 8, 197, 197]               0\n",
    "          Linear-105             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-106             [-1, 197, 768]               0\n",
    "         Dropout-107             [-1, 197, 768]               0\n",
    "     ResidualAdd-108             [-1, 197, 768]               0\n",
    "       LayerNorm-109             [-1, 197, 768]           1,536\n",
    "          Linear-110            [-1, 197, 3072]       2,362,368\n",
    "            GELU-111            [-1, 197, 3072]               0\n",
    "         Dropout-112            [-1, 197, 3072]               0\n",
    "          Linear-113             [-1, 197, 768]       2,360,064\n",
    "         Dropout-114             [-1, 197, 768]               0\n",
    "     ResidualAdd-115             [-1, 197, 768]               0\n",
    "       LayerNorm-116             [-1, 197, 768]           1,536\n",
    "          Linear-117            [-1, 197, 2304]       1,771,776\n",
    "         Dropout-118          [-1, 8, 197, 197]               0\n",
    "          Linear-119             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-120             [-1, 197, 768]               0\n",
    "         Dropout-121             [-1, 197, 768]               0\n",
    "     ResidualAdd-122             [-1, 197, 768]               0\n",
    "       LayerNorm-123             [-1, 197, 768]           1,536\n",
    "          Linear-124            [-1, 197, 3072]       2,362,368\n",
    "            GELU-125            [-1, 197, 3072]               0\n",
    "         Dropout-126            [-1, 197, 3072]               0\n",
    "          Linear-127             [-1, 197, 768]       2,360,064\n",
    "         Dropout-128             [-1, 197, 768]               0\n",
    "     ResidualAdd-129             [-1, 197, 768]               0\n",
    "       LayerNorm-130             [-1, 197, 768]           1,536\n",
    "          Linear-131            [-1, 197, 2304]       1,771,776\n",
    "         Dropout-132          [-1, 8, 197, 197]               0\n",
    "          Linear-133             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-134             [-1, 197, 768]               0\n",
    "         Dropout-135             [-1, 197, 768]               0\n",
    "     ResidualAdd-136             [-1, 197, 768]               0\n",
    "       LayerNorm-137             [-1, 197, 768]           1,536\n",
    "          Linear-138            [-1, 197, 3072]       2,362,368\n",
    "            GELU-139            [-1, 197, 3072]               0\n",
    "         Dropout-140            [-1, 197, 3072]               0\n",
    "          Linear-141             [-1, 197, 768]       2,360,064\n",
    "         Dropout-142             [-1, 197, 768]               0\n",
    "     ResidualAdd-143             [-1, 197, 768]               0\n",
    "       LayerNorm-144             [-1, 197, 768]           1,536\n",
    "          Linear-145            [-1, 197, 2304]       1,771,776\n",
    "         Dropout-146          [-1, 8, 197, 197]               0\n",
    "          Linear-147             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-148             [-1, 197, 768]               0\n",
    "         Dropout-149             [-1, 197, 768]               0\n",
    "     ResidualAdd-150             [-1, 197, 768]               0\n",
    "       LayerNorm-151             [-1, 197, 768]           1,536\n",
    "          Linear-152            [-1, 197, 3072]       2,362,368\n",
    "            GELU-153            [-1, 197, 3072]               0\n",
    "         Dropout-154            [-1, 197, 3072]               0\n",
    "          Linear-155             [-1, 197, 768]       2,360,064\n",
    "         Dropout-156             [-1, 197, 768]               0\n",
    "     ResidualAdd-157             [-1, 197, 768]               0\n",
    "       LayerNorm-158             [-1, 197, 768]           1,536\n",
    "          Linear-159            [-1, 197, 2304]       1,771,776\n",
    "         Dropout-160          [-1, 8, 197, 197]               0\n",
    "          Linear-161             [-1, 197, 768]         590,592\n",
    "MultiHeadAttention-162             [-1, 197, 768]               0\n",
    "         Dropout-163             [-1, 197, 768]               0\n",
    "     ResidualAdd-164             [-1, 197, 768]               0\n",
    "       LayerNorm-165             [-1, 197, 768]           1,536\n",
    "          Linear-166            [-1, 197, 3072]       2,362,368\n",
    "            GELU-167            [-1, 197, 3072]               0\n",
    "         Dropout-168            [-1, 197, 3072]               0\n",
    "          Linear-169             [-1, 197, 768]       2,360,064\n",
    "         Dropout-170             [-1, 197, 768]               0\n",
    "     ResidualAdd-171             [-1, 197, 768]               0\n",
    "          Reduce-172                  [-1, 768]               0\n",
    "       LayerNorm-173                  [-1, 768]           1,536\n",
    "          Linear-174                 [-1, 1000]         769,000\n",
    "================================================================\n",
    "Total params: 86,415,592\n",
    "Trainable params: 86,415,592\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.57\n",
    "Forward/backward pass size (MB): 364.33\n",
    "Params size (MB): 329.65\n",
    "Estimated Total Size (MB): 694.56\n",
    "----------------------------------------------------------------\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

